{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismic Facies Classification Using Deep Learning\n",
    "\n",
    "Implementation of:\n",
    "**\"A deep learning framework for seismic facies classification\"**  \n",
    "*Kaur et al., 2022, Interpretation*\n",
    "\n",
    "This notebook demonstrates the complete workflow:\n",
    "1. Data loading and preprocessing\n",
    "2. Model training (DeepLabv3+ and GAN)\n",
    "3. Model evaluation and testing\n",
    "4. Uncertainty estimation\n",
    "5. Visualization and comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from data_loader import (\n",
    "    SeismicFaciesDataset,\n",
    "    create_patches_from_volume,\n",
    "    get_dataloaders,\n",
    "    create_dummy_data\n",
    ")\n",
    "\n",
    "from model import (\n",
    "    DeepLabV3Plus,\n",
    "    GANSegmentation,\n",
    "    get_model\n",
    ")\n",
    "\n",
    "from train import (\n",
    "    Trainer,\n",
    "    train_model\n",
    ")\n",
    "\n",
    "from test import (\n",
    "    Tester,\n",
    "    test_model,\n",
    "    compare_models\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    evaluate_model,\n",
    "    estimate_uncertainty,\n",
    "    visualize_prediction,\n",
    "    plot_metrics,\n",
    "    plot_confusion_matrix\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set parameters according to the paper:\n",
    "- Patch size: 200 Ã— 200\n",
    "- Number of classes: 6 (facies types)\n",
    "- Batch size: 32\n",
    "- Epochs: 60\n",
    "- Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Data parameters\n",
    "    'patch_size': 200,          # As per paper\n",
    "    'num_classes': 6,           # As per paper (6 facies types)\n",
    "    'in_channels': 1,           # Grayscale seismic data\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 32,           # As per paper\n",
    "    'num_epochs': 60,           # As per paper (for GAN)\n",
    "    'learning_rate': 1e-4,      # Default Adam lr (not specified in paper)\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Uncertainty estimation\n",
    "    'num_mc_samples': 20,       # MC dropout samples (not specified, using common value)\n",
    "    \n",
    "    # Paths\n",
    "    'data_dir': './data',\n",
    "    'checkpoint_dir': './checkpoints',\n",
    "    'results_dir': './results'\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [CONFIG['data_dir'], CONFIG['checkpoint_dir'], CONFIG['results_dir']]:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "### Option A: Load Real Seismic Data\n",
    "\n",
    "If you have seismic data, load it here. Expected format:\n",
    "- Seismic data: numpy array of shape (N, 200, 200)\n",
    "- Labels: numpy array of shape (N, 200, 200) with values 0-5\n",
    "\n",
    "```python\n",
    "# Example:\n",
    "train_seismic = np.load('path/to/train_seismic.npy')\n",
    "train_labels = np.load('path/to/train_labels.npy')\n",
    "val_seismic = np.load('path/to/val_seismic.npy')\n",
    "val_labels = np.load('path/to/val_labels.npy')\n",
    "test_seismic = np.load('path/to/test_seismic.npy')\n",
    "test_labels = np.load('path/to/test_labels.npy')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Load your real data (uncomment and modify if you have data)\n",
    "# train_seismic = np.load('path/to/train_seismic.npy')\n",
    "# train_labels = np.load('path/to/train_labels.npy')\n",
    "# val_seismic = np.load('path/to/val_seismic.npy')\n",
    "# val_labels = np.load('path/to/val_labels.npy')\n",
    "# test_seismic = np.load('path/to/test_seismic.npy')\n",
    "# test_labels = np.load('path/to/test_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Generate Dummy Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Generate dummy data for demonstration\n",
    "print(\"Generating dummy seismic data...\")\n",
    "\n",
    "# According to paper: 27,648 training patches\n",
    "# For demo purposes, we use smaller numbers\n",
    "train_seismic, train_labels = create_dummy_data(\n",
    "    num_samples=1000,  # Use 27648 for full-scale training\n",
    "    patch_size=CONFIG['patch_size'],\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    save_path=CONFIG['data_dir']\n",
    ")\n",
    "\n",
    "val_seismic, val_labels = create_dummy_data(\n",
    "    num_samples=200,\n",
    "    patch_size=CONFIG['patch_size'],\n",
    "    num_classes=CONFIG['num_classes']\n",
    ")\n",
    "\n",
    "test_seismic, test_labels = create_dummy_data(\n",
    "    num_samples=200,\n",
    "    patch_size=CONFIG['patch_size'],\n",
    "    num_classes=CONFIG['num_classes']\n",
    ")\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  Train: {train_seismic.shape}\")\n",
    "print(f\"  Val:   {val_seismic.shape}\")\n",
    "print(f\"  Test:  {test_seismic.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "facies_names = [\n",
    "    'Basement rocks',\n",
    "    'Slope mudstone A',\n",
    "    'Mass-transport complex',\n",
    "    'Slope mudstone B',\n",
    "    'Slope valley',\n",
    "    'Submarine canyon'\n",
    "]\n",
    "\n",
    "for i in range(3):\n",
    "    # Seismic\n",
    "    axes[0, i].imshow(train_seismic[i], cmap='seismic', aspect='auto')\n",
    "    axes[0, i].set_title(f'Seismic Sample {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Labels\n",
    "    im = axes[1, i].imshow(train_labels[i], cmap='tab10', vmin=0, vmax=5, aspect='auto')\n",
    "    axes[1, i].set_title(f'Facies Labels {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(CONFIG['results_dir']) / 'sample_data.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFacies classes:\")\n",
    "for i, name in enumerate(facies_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    train_seismic, train_labels,\n",
    "    val_seismic, val_labels,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "\n",
    "test_dataset = SeismicFaciesDataset(test_seismic, test_labels, normalize=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "### 4.1 Train DeepLabv3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DeepLabv3+\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING DEEPLABV3+\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "deeplab_history = train_model(\n",
    "    model_type='deeplabv3+',\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=CONFIG['num_epochs'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    device=device,\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAN\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING GAN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gan_history = train_model(\n",
    "    model_type='gan',\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=CONFIG['num_epochs'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    device=device,\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(deeplab_history['train_loss'], label='DeepLabv3+ Train', linewidth=2)\n",
    "axes[0].plot(deeplab_history['val_loss'], label='DeepLabv3+ Val', linewidth=2, linestyle='--')\n",
    "axes[0].plot(gan_history['train_loss'], label='GAN Train', linewidth=2)\n",
    "axes[0].plot(gan_history['val_loss'], label='GAN Val', linewidth=2, linestyle='--')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# F1 score curves\n",
    "deeplab_train_f1 = [m['mean_f1'].item() for m in deeplab_history['train_metrics']]\n",
    "deeplab_val_f1 = [m['mean_f1'].item() for m in deeplab_history['val_metrics']]\n",
    "gan_train_f1 = [m['mean_f1'].item() for m in gan_history['train_metrics']]\n",
    "gan_val_f1 = [m['mean_f1'].item() for m in gan_history['val_metrics']]\n",
    "\n",
    "axes[1].plot(deeplab_train_f1, label='DeepLabv3+ Train', linewidth=2)\n",
    "axes[1].plot(deeplab_val_f1, label='DeepLabv3+ Val', linewidth=2, linestyle='--')\n",
    "axes[1].plot(gan_train_f1, label='GAN Train', linewidth=2)\n",
    "axes[1].plot(gan_val_f1, label='GAN Val', linewidth=2, linestyle='--')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[1].set_title('Training and Validation F1 Score', fontsize=14)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(CONFIG['results_dir']) / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "### 5.1 Evaluate DeepLabv3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DeepLabv3+\n",
    "deeplab_checkpoint = Path(CONFIG['checkpoint_dir']) / 'deeplabv3+_best.pth'\n",
    "\n",
    "deeplab_metrics = test_model(\n",
    "    model_type='deeplabv3+',\n",
    "    checkpoint_path=str(deeplab_checkpoint),\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    save_dir=Path(CONFIG['results_dir']) / 'deeplabv3+',\n",
    "    visualize=True,\n",
    "    estimate_uncertainty=True,\n",
    "    num_mc_samples=CONFIG['num_mc_samples']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluate GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GAN\n",
    "gan_checkpoint = Path(CONFIG['checkpoint_dir']) / 'gan_best.pth'\n",
    "\n",
    "gan_metrics = test_model(\n",
    "    model_type='gan',\n",
    "    checkpoint_path=str(gan_checkpoint),\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    save_dir=Path(CONFIG['results_dir']) / 'gan',\n",
    "    visualize=True,\n",
    "    estimate_uncertainty=True,\n",
    "    num_mc_samples=CONFIG['num_mc_samples']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Compare Models\n",
    "\n",
    "As described in the paper, we perform a comparative analysis of DeepLabv3+ and GAN results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models side-by-side\n",
    "comparison_results = compare_models(\n",
    "    deeplab_checkpoint=str(deeplab_checkpoint),\n",
    "    gan_checkpoint=str(gan_checkpoint),\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    save_dir=Path(CONFIG['results_dir']) / 'comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Uncertainty Analysis\n",
    "\n",
    "As described in the paper, we use Monte Carlo Dropout for epistemic uncertainty estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed uncertainty analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UNCERTAINTY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load model\n",
    "model = get_model('deeplabv3+', in_channels=1, num_classes=6)\n",
    "tester = Tester(model, 'deeplabv3+', device=device)\n",
    "tester.load_checkpoint(str(deeplab_checkpoint))\n",
    "\n",
    "# Get a batch for analysis\n",
    "seismic_batch, labels_batch = next(iter(test_loader))\n",
    "\n",
    "# Predict with uncertainty\n",
    "print(f\"\\nEstimating uncertainty with {CONFIG['num_mc_samples']} MC samples...\")\n",
    "predictions, uncertainty = tester.predict_with_uncertainty(\n",
    "    seismic_batch,\n",
    "    num_samples=CONFIG['num_mc_samples']\n",
    ")\n",
    "\n",
    "# Visualize uncertainty for first sample\n",
    "seismic_np = seismic_batch[0, 0].numpy()\n",
    "label_np = labels_batch[0].numpy()\n",
    "pred_np = predictions[0].numpy()\n",
    "uncert_np = uncertainty[0].numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "axes[0, 0].imshow(seismic_np, cmap='seismic', aspect='auto')\n",
    "axes[0, 0].set_title('Seismic Section', fontsize=14)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(label_np, cmap='tab10', vmin=0, vmax=5, aspect='auto')\n",
    "axes[0, 1].set_title('True Labels', fontsize=14)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(pred_np, cmap='tab10', vmin=0, vmax=5, aspect='auto')\n",
    "axes[1, 0].set_title('Predicted Labels', fontsize=14)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "im = axes[1, 1].imshow(uncert_np, cmap='hot', aspect='auto')\n",
    "axes[1, 1].set_title('Epistemic Uncertainty', fontsize=14)\n",
    "axes[1, 1].axis('off')\n",
    "plt.colorbar(im, ax=axes[1, 1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(CONFIG['results_dir']) / 'uncertainty_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nUncertainty statistics:\")\n",
    "print(f\"  Mean: {uncert_np.mean():.4f}\")\n",
    "print(f\"  Std:  {uncert_np.std():.4f}\")\n",
    "print(f\"  Min:  {uncert_np.min():.4f}\")\n",
    "print(f\"  Max:  {uncert_np.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Key Findings\n",
    "\n",
    "Based on the paper's findings:\n",
    "- **DeepLabv3+**: Produces sharper boundaries between facies\n",
    "- **GAN**: Better continuity of predicted facies\n",
    "- **Joint Analysis**: Combining predictions from both networks provides more accurate interpretation\n",
    "- **Uncertainty**: High uncertainty regions often correspond to facies boundaries or mispredicted areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDeepLabv3+ Performance:\")\n",
    "print(f\"  Mean Precision: {deeplab_metrics['mean_precision']:.4f}\")\n",
    "print(f\"  Mean Recall:    {deeplab_metrics['mean_recall']:.4f}\")\n",
    "print(f\"  Mean F1 Score:  {deeplab_metrics['mean_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nGAN Performance:\")\n",
    "print(f\"  Mean Precision: {gan_metrics['mean_precision']:.4f}\")\n",
    "print(f\"  Mean Recall:    {gan_metrics['mean_recall']:.4f}\")\n",
    "print(f\"  Mean F1 Score:  {gan_metrics['mean_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  - DeepLabv3+ captures sharper facies boundaries (ASPP + encoder-decoder)\")\n",
    "print(\"  - GAN provides better facies continuity (adversarial training)\")\n",
    "print(\"  - Uncertainty is higher at facies boundaries and mispredicted regions\")\n",
    "print(\"  - Joint analysis of both models recommended for interpretation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"All results saved to: {CONFIG['results_dir']}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference on New Data (Optional)\n",
    "\n",
    "Use trained models to predict facies on new seismic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict on a new seismic volume\n",
    "# Uncomment and modify if you have new data to predict\n",
    "\n",
    "# # Load new seismic data\n",
    "# new_seismic = np.load('path/to/new_seismic.npy')  # Shape: (N, 200, 200)\n",
    "\n",
    "# # Predict using DeepLabv3+\n",
    "# model = get_model('deeplabv3+', in_channels=1, num_classes=6)\n",
    "# tester = Tester(model, 'deeplabv3+', device=device)\n",
    "# tester.load_checkpoint(str(deeplab_checkpoint))\n",
    "\n",
    "# predictions, uncertainty = tester.predict_full_volume(\n",
    "#     new_seismic,\n",
    "#     batch_size=CONFIG['batch_size'],\n",
    "#     estimate_uncertainty_flag=True,\n",
    "#     num_mc_samples=CONFIG['num_mc_samples']\n",
    "# )\n",
    "\n",
    "# # Save predictions\n",
    "# np.save(Path(CONFIG['results_dir']) / 'predictions.npy', predictions)\n",
    "# np.save(Path(CONFIG['results_dir']) / 'uncertainty.npy', uncertainty)\n",
    "\n",
    "# print(f\"Predictions saved to {CONFIG['results_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "Kaur, H., Pham, N., Fomel, S., Geng, Z., Decker, L., Gremillion, B., Jervis, M., Abma, R., & Gao, S. (2022). A deep learning framework for seismic facies classification. *Interpretation*, 11(1), T107-T116.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
